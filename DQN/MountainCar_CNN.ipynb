{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_2___Final_512_Batch (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgdHRrjOlKeX"
      },
      "source": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wUVGalnEeju"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "import base64\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSYhGkMwEq9w"
      },
      "source": [
        "'''class DQN_MountainCar(nn.Module):\n",
        "    def __init__(self, img_height, img_width):\n",
        "        super().__init__()\n",
        "         \n",
        "        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=64)   \n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=128)\n",
        "        self.out = nn.Linear(in_features=128, out_features=3)            \n",
        "\n",
        "    def forward(self, t):\n",
        "        t = t.flatten(start_dim=1)\n",
        "        t = F.relu(self.fc1(t))\n",
        "        t = F.relu(self.fc2(t))\n",
        "        t = self.out(t)\n",
        "        return t'''\n",
        "class DQN_MountainCar(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN_MountainCar, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2)\n",
        "\n",
        "        # Number of Linear input connections depends on output of conv2d layers\n",
        "        # and therefore the input image size, so compute it.\n",
        "        def conv2d_size_out(size, kernel_size = 4, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 128\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        return self.head(x.view(x.size(0), -1))\n",
        "Transition = namedtuple('transition',('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.push_count = 0\n",
        "        \n",
        "    def push(self, experience):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(experience)\n",
        "        else:\n",
        "            self.memory[self.push_count % self.capacity] = experience\n",
        "        self.push_count += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def can_provide_sample(self, batch_size):\n",
        "        return len(self.memory) >= batch_size\n",
        "\n",
        "class EpsilonGreedy():\n",
        "    def __init__(self, start, end, decay):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.decay = decay\n",
        "    \n",
        "    def get_exploration_rate(self, current_step):\n",
        "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, method, num_actions, device):\n",
        "        self.current_step = 0\n",
        "        self.num_actions = num_actions\n",
        "        self.device = device\n",
        "        self.method = method\n",
        "\n",
        "    def select_action(self, state, policy):\n",
        "        rate = self.method.get_exploration_rate(self.current_step)\n",
        "        self.current_step += 1\n",
        "\n",
        "        if rate > random.random():\n",
        "            action = random.randrange(self.num_actions)\n",
        "            return torch.tensor([action]).to(self.device)    \n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return policy(state).argmax(dim=1).to(self.device)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhOpk1DLFn2a"
      },
      "source": [
        "class MountainCarManager():\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        self.env = wrap_env(gym.make('MountainCar-v0'))\n",
        "        self.env.reset()\n",
        "        self.CurrScreen = None\n",
        "        self.done = False\n",
        "    \n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        self.CurrScreen = None\n",
        "        \n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        return self.env.render(mode)\n",
        "\n",
        "    def getState(self):\n",
        "        if self.isStartingScreen() or self.done:\n",
        "            self.CurrScreen = self.processedScreen()\n",
        "            black_screen = torch.zeros_like(self.CurrScreen)\n",
        "            return black_screen\n",
        "        else:\n",
        "            screen1 = self.CurrScreen\n",
        "            screen2 = self.processedScreen()\n",
        "            self.CurrScreen = screen2\n",
        "            return screen2 - screen1\n",
        "\n",
        "\n",
        "    def availableActions(self):\n",
        "        return self.env.action_space.n\n",
        "        \n",
        "    def takeAction(self, action):        \n",
        "        next_state, reward, self.done, _ = self.env.step(action.item())\n",
        "        return next_state, torch.tensor([reward], device=self.device)\n",
        "\n",
        "    def getHeightOfScreen(self):\n",
        "        screen = self.processedScreen()\n",
        "        return screen.shape[2]\n",
        "    \n",
        "    def getWidth(self):\n",
        "        screen = self.processedScreen()\n",
        "        return screen.shape[3]\n",
        "       \n",
        "    def processedScreen(self):\n",
        "        screen = self.render('rgb_array').transpose((2, 0, 1))\n",
        "        return self.screenValidTransformation(screen)\n",
        "\n",
        "    def isStartingScreen(self):\n",
        "        return self.CurrScreen is None\n",
        "    \n",
        "    \n",
        "    def screenValidTransformation(self, screen):       \n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        \n",
        "        resize = T.Compose([T.ToPILImage(),T.Resize((60,100)),T.ToTensor()])\n",
        "        \n",
        "        return resize(screen).unsqueeze(0).to(self.device) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHrFgNurF9vE"
      },
      "source": [
        "def getTensors(experiences):\n",
        "    # Convert batch of Transitions to Transition of batches\n",
        "    batch = Transition(*zip(*experiences))\n",
        "\n",
        "    state = torch.cat(batch.state)\n",
        "    action = torch.cat(batch.action)\n",
        "    reward = torch.cat(batch.reward)\n",
        "    next_state = torch.cat(batch.next_state)\n",
        "\n",
        "    return (state,action,reward,next_state)\n",
        "\n",
        "\n",
        "class findQVals():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    @staticmethod\n",
        "    def getCurr(policy, states, actions):\n",
        "        return policy(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
        "\n",
        "    @staticmethod    \n",
        "    def getNextState(target, next_states):                \n",
        "        finalStates = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)\n",
        "        nonFinal = (finalStates == False)\n",
        "        non_final_states = next_states[nonFinal]\n",
        "        batch_size = next_states.shape[0]\n",
        "        values = torch.zeros(batch_size).to(findQVals.device)\n",
        "        values[nonFinal] = target(non_final_states).max(dim=1)[0].detach()\n",
        "        return values\n",
        "\n",
        "\n",
        "def getReward(location, velocity):\n",
        "  reward = 0\n",
        "  if location > 0.51:\n",
        "    reward += 2500\n",
        "  if location > 0.501:\n",
        "    reward += 2000\n",
        "  if location > 0.5:\n",
        "    reward += 1000\n",
        "  if location > 0.45:\n",
        "    reward += 200\n",
        "  if location > 0.4:\n",
        "    reward += 70\n",
        "  if location > 0.3:\n",
        "    reward += 15\n",
        "  if location > 0.1:\n",
        "    reward += 7\n",
        "  if location > -0.1:\n",
        "    reward += 4\n",
        "  if location > -0.3:\n",
        "    reward += 2\n",
        "  if location > -0.4:\n",
        "    reward += 0.5\n",
        "\n",
        "\n",
        "  if abs(velocity) > 0.03:\n",
        "    reward += 12\n",
        "  if abs(velocity) > 0.02:\n",
        "    reward += 7\n",
        "  if abs(velocity) < 0.005:\n",
        "    reward -= 2\n",
        "\n",
        "  return reward"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHECZur_GIE3",
        "outputId": "e197ce11-5b5e-4747-e6ae-57e9600fc32b"
      },
      "source": [
        "batch_size = 256\n",
        "gamma = 0.99\n",
        "epsilonStart = 1\n",
        "epsilonEnd = 0.01\n",
        "epsilonDecay = 200\n",
        "targetUpdate = 10\n",
        "memory_size = 100000\n",
        "lr = 0.002\n",
        "num_episodes = 1100\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "em = MountainCarManager(device)\n",
        "strategy = EpsilonGreedy(epsilonStart, epsilonEnd, epsilonDecay)\n",
        "agent = Agent(strategy, em.availableActions(), device)\n",
        "memory = ReplayMemory(memory_size)\n",
        "policy = DQN_MountainCar(em.getHeightOfScreen(), em.getWidth() , 3).to(device)\n",
        "target = DQN_MountainCar(em.getHeightOfScreen(), em.getWidth(), 3).to(device)\n",
        "target.load_state_dict(policy.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "target.eval()\n",
        "optimizer = optim.Adam(params=policy.parameters(), lr=lr)\n",
        "maxRightValues = []\n",
        "episode_rewards = []\n",
        "episodeNum = []\n",
        "trainingTime = time.time()\n",
        "for episode in range(num_episodes):\n",
        "    em.reset()\n",
        "    state = em.getState()\n",
        "    episodeReward = 0\n",
        "    maxRight = -1.2\n",
        "\n",
        "    for timestep in count():\n",
        "        action = agent.select_action(state, policy)\n",
        "        tempState, reward = em.takeAction(action)\n",
        "        next_state = em.getState()\n",
        "        reward = getReward(tempState[0],tempState[1])\n",
        "        right = tempState[0]\n",
        "        episodeReward += reward\n",
        "\n",
        "        if maxRight < right:\n",
        "          maxRight = right\n",
        "          if maxRight >= 0.5:\n",
        "            print(\"EPISODE WON\", episode)\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        memory.push(Transition(state, action, next_state, reward))\n",
        "        state = next_state\n",
        "\n",
        "        if memory.can_provide_sample(batch_size):\n",
        "            experiences = memory.sample(batch_size)\n",
        "            states, actions, rewards, next_states = getTensors(experiences)\n",
        "            \n",
        "            currentQ = findQVals.getCurr(policy, states, actions)\n",
        "            nextQ = findQVals.getNextState(target, next_states)\n",
        "            targetQ = (nextQ * gamma) + rewards\n",
        "\n",
        "            loss = F.mse_loss(currentQ, targetQ.unsqueeze(1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        if em.done:\n",
        "            episodeNum.append(timestep)\n",
        "            break\n",
        "\n",
        "    print(\"Episode: \", episode, \"Max Right: \", round(maxRight, 3), \"Reward: \", round(episodeReward,3))\n",
        "    maxRightValues.append(maxRight)\n",
        "    episode_rewards.append(episodeReward)\n",
        "    if episode % targetUpdate == 0:\n",
        "        target.load_state_dict(policy.state_dict())\n",
        "        \n",
        "em.close()\n",
        "trainingTime = time.time() - trainingTime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Episode:  0 Max Right:  -0.414 Reward:  -140\n",
            "Episode:  1 Max Right:  -0.226 Reward:  3.5\n",
            "Episode:  2 Max Right:  -0.305 Reward:  -94.5\n",
            "Episode:  3 Max Right:  -0.245 Reward:  202.5\n",
            "Episode:  4 Max Right:  -0.272 Reward:  295.5\n",
            "Episode:  5 Max Right:  -0.278 Reward:  175.0\n",
            "Episode:  6 Max Right:  -0.456 Reward:  -376\n",
            "Episode:  7 Max Right:  -0.268 Reward:  37.5\n",
            "Episode:  8 Max Right:  -0.271 Reward:  244.5\n",
            "Episode:  9 Max Right:  -0.278 Reward:  -61.5\n",
            "Episode:  10 Max Right:  -0.304 Reward:  -133.0\n",
            "Episode:  11 Max Right:  -0.242 Reward:  807.5\n",
            "Episode:  12 Max Right:  -0.203 Reward:  953.0\n",
            "Episode:  13 Max Right:  -0.167 Reward:  1052.0\n",
            "Episode:  14 Max Right:  -0.19 Reward:  712.0\n",
            "Episode:  15 Max Right:  -0.124 Reward:  854.0\n",
            "Episode:  16 Max Right:  -0.198 Reward:  637.0\n",
            "Episode:  17 Max Right:  -0.146 Reward:  658.0\n",
            "Episode:  18 Max Right:  -0.166 Reward:  990.0\n",
            "Episode:  19 Max Right:  -0.171 Reward:  275.5\n",
            "Episode:  20 Max Right:  -0.106 Reward:  1100.0\n",
            "Episode:  21 Max Right:  -0.111 Reward:  786.0\n",
            "Episode:  22 Max Right:  -0.16 Reward:  976.5\n",
            "Episode:  23 Max Right:  -0.034 Reward:  1626.0\n",
            "Episode:  24 Max Right:  0.007 Reward:  1232.0\n",
            "Episode:  25 Max Right:  -0.092 Reward:  858.5\n",
            "Episode:  26 Max Right:  0.077 Reward:  1119.5\n",
            "Episode:  27 Max Right:  0.069 Reward:  1222.0\n",
            "Episode:  28 Max Right:  0.038 Reward:  1737.5\n",
            "Episode:  29 Max Right:  0.041 Reward:  1310.0\n",
            "Episode:  30 Max Right:  0.065 Reward:  1210.0\n",
            "Episode:  31 Max Right:  0.086 Reward:  1404.0\n",
            "Episode:  32 Max Right:  0.07 Reward:  1438.0\n",
            "Episode:  33 Max Right:  -0.027 Reward:  1345.0\n",
            "Episode:  34 Max Right:  0.105 Reward:  1554.5\n",
            "Episode:  35 Max Right:  0.131 Reward:  1608.5\n",
            "Episode:  36 Max Right:  0.139 Reward:  1450.0\n",
            "Episode:  37 Max Right:  0.154 Reward:  1568.0\n",
            "Episode:  38 Max Right:  0.155 Reward:  1695.5\n",
            "Episode:  39 Max Right:  0.116 Reward:  1633.5\n",
            "Episode:  40 Max Right:  0.065 Reward:  1419.5\n",
            "Episode:  41 Max Right:  0.204 Reward:  1751.0\n",
            "Episode:  42 Max Right:  0.067 Reward:  1856.5\n",
            "Episode:  43 Max Right:  0.143 Reward:  2072.0\n",
            "EPISODE WON 44\n",
            "Episode:  44 Max Right:  0.503 Reward:  6922.5\n",
            "Episode:  45 Max Right:  0.182 Reward:  1730.0\n",
            "Episode:  46 Max Right:  0.296 Reward:  1721.0\n",
            "Episode:  47 Max Right:  0.157 Reward:  1813.0\n",
            "Episode:  48 Max Right:  0.212 Reward:  1785.5\n",
            "Episode:  49 Max Right:  0.106 Reward:  1691.5\n",
            "Episode:  50 Max Right:  -0.084 Reward:  716.5\n",
            "Episode:  51 Max Right:  0.11 Reward:  1678.0\n",
            "Episode:  52 Max Right:  0.214 Reward:  1808.5\n",
            "Episode:  53 Max Right:  0.125 Reward:  1118.5\n",
            "Episode:  54 Max Right:  0.301 Reward:  1796.0\n",
            "Episode:  55 Max Right:  0.237 Reward:  1541.5\n",
            "Episode:  56 Max Right:  0.369 Reward:  2144.5\n",
            "Episode:  57 Max Right:  0.347 Reward:  1710.5\n",
            "Episode:  58 Max Right:  0.128 Reward:  1298.0\n",
            "Episode:  59 Max Right:  0.273 Reward:  1680.5\n",
            "Episode:  60 Max Right:  0.491 Reward:  9559.5\n",
            "Episode:  61 Max Right:  0.2 Reward:  1715.0\n",
            "Episode:  62 Max Right:  0.438 Reward:  3625.0\n",
            "Episode:  63 Max Right:  -0.196 Reward:  686.5\n",
            "Episode:  64 Max Right:  0.125 Reward:  1674.5\n",
            "Episode:  65 Max Right:  0.234 Reward:  1388.0\n",
            "EPISODE WON 66\n",
            "Episode:  66 Max Right:  0.502 Reward:  6256.0\n",
            "Episode:  67 Max Right:  0.272 Reward:  1816.0\n",
            "Episode:  68 Max Right:  0.289 Reward:  1266.0\n",
            "Episode:  69 Max Right:  0.237 Reward:  1325.5\n",
            "Episode:  70 Max Right:  0.217 Reward:  1701.0\n",
            "Episode:  71 Max Right:  0.069 Reward:  1401.5\n",
            "Episode:  72 Max Right:  0.01 Reward:  1339.0\n",
            "Episode:  73 Max Right:  0.311 Reward:  2019.0\n",
            "Episode:  74 Max Right:  0.332 Reward:  1867.5\n",
            "Episode:  75 Max Right:  0.326 Reward:  2086.0\n",
            "Episode:  76 Max Right:  0.221 Reward:  1967.0\n",
            "Episode:  77 Max Right:  0.321 Reward:  2001.5\n",
            "Episode:  78 Max Right:  0.309 Reward:  2450.5\n",
            "EPISODE WON 79\n",
            "Episode:  79 Max Right:  0.502 Reward:  5473.5\n",
            "Episode:  80 Max Right:  0.139 Reward:  952.0\n",
            "Episode:  81 Max Right:  0.007 Reward:  382.5\n",
            "Episode:  82 Max Right:  0.459 Reward:  6834.5\n",
            "EPISODE WON 83\n",
            "Episode:  83 Max Right:  0.506 Reward:  6248.0\n",
            "Episode:  84 Max Right:  0.307 Reward:  1840.5\n",
            "Episode:  85 Max Right:  0.204 Reward:  1165.5\n",
            "Episode:  86 Max Right:  0.01 Reward:  1263.5\n",
            "Episode:  87 Max Right:  0.196 Reward:  1496.5\n",
            "Episode:  88 Max Right:  0.214 Reward:  1249.5\n",
            "EPISODE WON 89\n",
            "Episode:  89 Max Right:  0.511 Reward:  9202.0\n",
            "Episode:  90 Max Right:  0.25 Reward:  1790.5\n",
            "Episode:  91 Max Right:  -0.008 Reward:  709.0\n",
            "Episode:  92 Max Right:  0.224 Reward:  1847.0\n",
            "EPISODE WON 93\n",
            "Episode:  93 Max Right:  0.512 Reward:  8049.0\n",
            "EPISODE WON 94\n",
            "Episode:  94 Max Right:  0.502 Reward:  8651.0\n",
            "EPISODE WON 95\n",
            "Episode:  95 Max Right:  0.501 Reward:  3763.5\n",
            "Episode:  96 Max Right:  0.214 Reward:  922.5\n",
            "Episode:  97 Max Right:  0.471 Reward:  2166.0\n",
            "Episode:  98 Max Right:  0.322 Reward:  973.5\n",
            "EPISODE WON 99\n",
            "Episode:  99 Max Right:  0.505 Reward:  6723.5\n",
            "EPISODE WON 100\n",
            "Episode:  100 Max Right:  0.503 Reward:  5791.5\n",
            "Episode:  101 Max Right:  0.106 Reward:  958.5\n",
            "EPISODE WON 102\n",
            "Episode:  102 Max Right:  0.523 Reward:  8591.5\n",
            "EPISODE WON 103\n",
            "Episode:  103 Max Right:  0.503 Reward:  6942.5\n",
            "Episode:  104 Max Right:  0.499 Reward:  3118.5\n",
            "EPISODE WON 105\n",
            "Episode:  105 Max Right:  0.514 Reward:  7703.0\n",
            "Episode:  106 Max Right:  0.161 Reward:  1105.5\n",
            "Episode:  107 Max Right:  -0.1 Reward:  947.5\n",
            "Episode:  108 Max Right:  0.167 Reward:  1628.0\n",
            "EPISODE WON 109\n",
            "Episode:  109 Max Right:  0.506 Reward:  5548.0\n",
            "Episode:  110 Max Right:  0.261 Reward:  1927.5\n",
            "Episode:  111 Max Right:  -0.031 Reward:  868.0\n",
            "Episode:  112 Max Right:  -0.462 Reward:  -314\n",
            "Episode:  113 Max Right:  0.217 Reward:  1655.5\n",
            "Episode:  114 Max Right:  0.304 Reward:  2058.0\n",
            "EPISODE WON 115\n",
            "Episode:  115 Max Right:  0.512 Reward:  8816.5\n",
            "Episode:  116 Max Right:  0.038 Reward:  1412.5\n",
            "EPISODE WON 117\n",
            "Episode:  117 Max Right:  0.509 Reward:  5672.0\n",
            "Episode:  118 Max Right:  0.339 Reward:  2228.0\n",
            "Episode:  119 Max Right:  0.204 Reward:  1953.5\n",
            "EPISODE WON 120\n",
            "Episode:  120 Max Right:  0.513 Reward:  8675.0\n",
            "Episode:  121 Max Right:  0.262 Reward:  1941.0\n",
            "Episode:  122 Max Right:  0.103 Reward:  1518.0\n",
            "Episode:  123 Max Right:  -0.166 Reward:  242.0\n",
            "EPISODE WON 124\n",
            "Episode:  124 Max Right:  0.521 Reward:  8316.5\n",
            "Episode:  125 Max Right:  0.29 Reward:  1801.5\n",
            "Episode:  126 Max Right:  0.092 Reward:  1645.0\n",
            "Episode:  127 Max Right:  0.196 Reward:  1934.5\n",
            "Episode:  128 Max Right:  0.449 Reward:  3711.5\n",
            "Episode:  129 Max Right:  0.269 Reward:  2101.5\n",
            "EPISODE WON 130\n",
            "Episode:  130 Max Right:  0.503 Reward:  7035.0\n",
            "Episode:  131 Max Right:  0.393 Reward:  1914.5\n",
            "EPISODE WON 132\n",
            "Episode:  132 Max Right:  0.503 Reward:  6643.0\n",
            "Episode:  133 Max Right:  0.246 Reward:  1615.0\n",
            "Episode:  134 Max Right:  0.288 Reward:  1817.5\n",
            "Episode:  135 Max Right:  0.032 Reward:  1405.0\n",
            "Episode:  136 Max Right:  0.151 Reward:  1550.0\n",
            "EPISODE WON 137\n",
            "Episode:  137 Max Right:  0.501 Reward:  4991.5\n",
            "EPISODE WON 138\n",
            "Episode:  138 Max Right:  0.506 Reward:  5798.5\n",
            "EPISODE WON 139\n",
            "Episode:  139 Max Right:  0.5 Reward:  3763.5\n",
            "EPISODE WON 140\n",
            "Episode:  140 Max Right:  0.506 Reward:  6370.0\n",
            "Episode:  141 Max Right:  0.237 Reward:  1651.5\n",
            "EPISODE WON 142\n",
            "Episode:  142 Max Right:  0.504 Reward:  5645.0\n",
            "EPISODE WON 143\n",
            "Episode:  143 Max Right:  0.505 Reward:  6246.5\n",
            "Episode:  144 Max Right:  0.089 Reward:  1530.0\n",
            "EPISODE WON 145\n",
            "Episode:  145 Max Right:  0.507 Reward:  5795.0\n",
            "EPISODE WON 146\n",
            "Episode:  146 Max Right:  0.514 Reward:  8134.5\n",
            "Episode:  147 Max Right:  0.3 Reward:  1906.0\n",
            "Episode:  148 Max Right:  0.334 Reward:  2264.5\n",
            "EPISODE WON 149\n",
            "Episode:  149 Max Right:  0.51 Reward:  8122.0\n",
            "Episode:  150 Max Right:  0.301 Reward:  1864.5\n",
            "Episode:  151 Max Right:  0.242 Reward:  1794.0\n",
            "Episode:  152 Max Right:  -0.044 Reward:  644.0\n",
            "EPISODE WON 153\n",
            "Episode:  153 Max Right:  0.518 Reward:  8633.0\n",
            "EPISODE WON 154\n",
            "Episode:  154 Max Right:  0.503 Reward:  6826.5\n",
            "EPISODE WON 155\n",
            "Episode:  155 Max Right:  0.504 Reward:  7612.5\n",
            "EPISODE WON 156\n",
            "Episode:  156 Max Right:  0.506 Reward:  6146.0\n",
            "EPISODE WON 157\n",
            "Episode:  157 Max Right:  0.509 Reward:  5692.5\n",
            "Episode:  158 Max Right:  0.187 Reward:  1447.0\n",
            "EPISODE WON 159\n",
            "Episode:  159 Max Right:  0.515 Reward:  8204.5\n",
            "EPISODE WON 160\n",
            "Episode:  160 Max Right:  0.502 Reward:  5313.5\n",
            "Episode:  161 Max Right:  0.123 Reward:  1580.5\n",
            "EPISODE WON 162\n",
            "Episode:  162 Max Right:  0.502 Reward:  6399.5\n",
            "EPISODE WON 163\n",
            "Episode:  163 Max Right:  0.509 Reward:  6117.5\n",
            "Episode:  164 Max Right:  0.438 Reward:  2156.0\n",
            "Episode:  165 Max Right:  0.087 Reward:  1448.5\n",
            "Episode:  166 Max Right:  0.107 Reward:  1340.5\n",
            "Episode:  167 Max Right:  0.092 Reward:  1661.5\n",
            "EPISODE WON 168\n",
            "Episode:  168 Max Right:  0.514 Reward:  8527.5\n",
            "EPISODE WON 169\n",
            "Episode:  169 Max Right:  0.522 Reward:  8620.0\n",
            "Episode:  170 Max Right:  0.052 Reward:  1221.5\n",
            "Episode:  171 Max Right:  0.127 Reward:  1811.5\n",
            "Episode:  172 Max Right:  0.186 Reward:  1259.5\n",
            "Episode:  173 Max Right:  0.188 Reward:  1178.0\n",
            "Episode:  174 Max Right:  0.242 Reward:  1759.0\n",
            "Episode:  175 Max Right:  0.225 Reward:  1537.5\n",
            "EPISODE WON 176\n",
            "Episode:  176 Max Right:  0.508 Reward:  5962.5\n",
            "Episode:  177 Max Right:  0.061 Reward:  1288.5\n",
            "Episode:  178 Max Right:  0.163 Reward:  1460.5\n",
            "EPISODE WON 179\n",
            "Episode:  179 Max Right:  0.511 Reward:  9283.5\n",
            "EPISODE WON 180\n",
            "Episode:  180 Max Right:  0.513 Reward:  8824.5\n",
            "Episode:  181 Max Right:  0.217 Reward:  1309.5\n",
            "Episode:  182 Max Right:  -0.086 Reward:  1130.0\n",
            "Episode:  183 Max Right:  0.157 Reward:  1828.0\n",
            "EPISODE WON 184\n",
            "Episode:  184 Max Right:  0.508 Reward:  6659.0\n",
            "Episode:  185 Max Right:  0.145 Reward:  2155.5\n",
            "EPISODE WON 186\n",
            "Episode:  186 Max Right:  0.507 Reward:  5948.5\n",
            "Episode:  187 Max Right:  0.16 Reward:  1740.0\n",
            "EPISODE WON 188\n",
            "Episode:  188 Max Right:  0.506 Reward:  6579.0\n",
            "Episode:  189 Max Right:  0.11 Reward:  1175.0\n",
            "Episode:  190 Max Right:  0.175 Reward:  1673.0\n",
            "Episode:  191 Max Right:  0.025 Reward:  937.0\n",
            "Episode:  192 Max Right:  0.04 Reward:  1307.0\n",
            "Episode:  193 Max Right:  0.177 Reward:  1759.5\n",
            "Episode:  194 Max Right:  0.241 Reward:  1796.5\n",
            "Episode:  195 Max Right:  0.162 Reward:  1331.0\n",
            "Episode:  196 Max Right:  0.151 Reward:  1649.0\n",
            "Episode:  197 Max Right:  0.088 Reward:  1503.5\n",
            "EPISODE WON 198\n",
            "Episode:  198 Max Right:  0.52 Reward:  8804.0\n",
            "EPISODE WON 199\n",
            "Episode:  199 Max Right:  0.501 Reward:  4741.0\n",
            "EPISODE WON 200\n",
            "Episode:  200 Max Right:  0.51 Reward:  8075.0\n",
            "Episode:  201 Max Right:  0.042 Reward:  248.5\n",
            "Episode:  202 Max Right:  0.011 Reward:  816.0\n",
            "Episode:  203 Max Right:  0.115 Reward:  1452.5\n",
            "Episode:  204 Max Right:  0.145 Reward:  1296.5\n",
            "EPISODE WON 205\n",
            "Episode:  205 Max Right:  0.511 Reward:  8635.0\n",
            "EPISODE WON 206\n",
            "Episode:  206 Max Right:  0.505 Reward:  5893.5\n",
            "Episode:  207 Max Right:  0.152 Reward:  1812.0\n",
            "Episode:  208 Max Right:  0.439 Reward:  1998.5\n",
            "EPISODE WON 209\n",
            "Episode:  209 Max Right:  0.503 Reward:  6522.5\n",
            "Episode:  210 Max Right:  0.129 Reward:  1790.0\n",
            "Episode:  211 Max Right:  0.096 Reward:  906.5\n",
            "Episode:  212 Max Right:  0.009 Reward:  1330.5\n",
            "Episode:  213 Max Right:  0.195 Reward:  2028.5\n",
            "Episode:  214 Max Right:  0.159 Reward:  1845.0\n",
            "EPISODE WON 215\n",
            "Episode:  215 Max Right:  0.512 Reward:  8445.5\n",
            "Episode:  216 Max Right:  0.328 Reward:  1678.0\n",
            "Episode:  217 Max Right:  0.436 Reward:  1845.0\n",
            "Episode:  218 Max Right:  0.144 Reward:  1603.5\n",
            "Episode:  219 Max Right:  0.16 Reward:  1686.5\n",
            "Episode:  220 Max Right:  0.053 Reward:  1071.5\n",
            "Episode:  221 Max Right:  -0.049 Reward:  688.5\n",
            "Episode:  222 Max Right:  0.12 Reward:  1327.0\n",
            "EPISODE WON 223\n",
            "Episode:  223 Max Right:  0.505 Reward:  6230.5\n",
            "EPISODE WON 224\n",
            "Episode:  224 Max Right:  0.511 Reward:  8865.5\n",
            "Episode:  225 Max Right:  0.075 Reward:  1365.5\n",
            "EPISODE WON 226\n",
            "Episode:  226 Max Right:  0.525 Reward:  8744.5\n",
            "Episode:  227 Max Right:  -0.07 Reward:  1081.0\n",
            "Episode:  228 Max Right:  0.02 Reward:  826.0\n",
            "Episode:  229 Max Right:  0.15 Reward:  1167.0\n",
            "Episode:  230 Max Right:  0.114 Reward:  1475.5\n",
            "EPISODE WON 231\n",
            "Episode:  231 Max Right:  0.516 Reward:  8234.0\n",
            "Episode:  232 Max Right:  0.232 Reward:  1709.5\n",
            "Episode:  233 Max Right:  -0.128 Reward:  473.5\n",
            "EPISODE WON 234\n",
            "Episode:  234 Max Right:  0.502 Reward:  6785.5\n",
            "Episode:  235 Max Right:  0.162 Reward:  1471.0\n",
            "Episode:  236 Max Right:  0.15 Reward:  1650.5\n",
            "Episode:  237 Max Right:  0.456 Reward:  2383.5\n",
            "EPISODE WON 238\n",
            "Episode:  238 Max Right:  0.511 Reward:  8622.0\n",
            "Episode:  239 Max Right:  0.431 Reward:  1428.0\n",
            "Episode:  240 Max Right:  0.438 Reward:  3608.0\n",
            "EPISODE WON 241\n",
            "Episode:  241 Max Right:  0.516 Reward:  8329.0\n",
            "Episode:  242 Max Right:  -0.015 Reward:  786.5\n",
            "Episode:  243 Max Right:  0.203 Reward:  1539.0\n",
            "Episode:  244 Max Right:  0.248 Reward:  1629.5\n",
            "Episode:  245 Max Right:  -0.138 Reward:  393.0\n",
            "Episode:  246 Max Right:  -0.215 Reward:  101.5\n",
            "Episode:  247 Max Right:  0.131 Reward:  1535.5\n",
            "EPISODE WON 248\n",
            "Episode:  248 Max Right:  0.507 Reward:  5822.5\n",
            "EPISODE WON 249\n",
            "Episode:  249 Max Right:  0.501 Reward:  3588.0\n",
            "Episode:  250 Max Right:  0.238 Reward:  1206.5\n",
            "Episode:  251 Max Right:  0.499 Reward:  2759.5\n",
            "Episode:  252 Max Right:  0.066 Reward:  1010.5\n",
            "Episode:  253 Max Right:  0.376 Reward:  1993.0\n",
            "Episode:  254 Max Right:  0.15 Reward:  1548.0\n",
            "EPISODE WON 255\n",
            "Episode:  255 Max Right:  0.512 Reward:  8695.0\n",
            "EPISODE WON 256\n",
            "Episode:  256 Max Right:  0.509 Reward:  6548.5\n",
            "Episode:  257 Max Right:  0.374 Reward:  1717.5\n",
            "Episode:  258 Max Right:  0.127 Reward:  1346.0\n",
            "EPISODE WON 259\n",
            "Episode:  259 Max Right:  0.504 Reward:  6294.0\n",
            "EPISODE WON 260\n",
            "Episode:  260 Max Right:  0.503 Reward:  6429.5\n",
            "Episode:  261 Max Right:  0.194 Reward:  1591.0\n",
            "Episode:  262 Max Right:  0.426 Reward:  1988.0\n",
            "Episode:  263 Max Right:  0.204 Reward:  1942.5\n",
            "EPISODE WON 264\n",
            "Episode:  264 Max Right:  0.507 Reward:  6691.0\n",
            "Episode:  265 Max Right:  -0.007 Reward:  1158.5\n",
            "Episode:  266 Max Right:  0.44 Reward:  2149.5\n",
            "Episode:  267 Max Right:  -0.034 Reward:  945.0\n",
            "EPISODE WON 268\n",
            "Episode:  268 Max Right:  0.512 Reward:  8919.5\n",
            "EPISODE WON 269\n",
            "Episode:  269 Max Right:  0.502 Reward:  6153.0\n",
            "Episode:  270 Max Right:  0.247 Reward:  1687.5\n",
            "EPISODE WON 271\n",
            "Episode:  271 Max Right:  0.504 Reward:  6448.0\n",
            "Episode:  272 Max Right:  0.087 Reward:  1344.5\n",
            "EPISODE WON 273\n",
            "Episode:  273 Max Right:  0.501 Reward:  5009.0\n",
            "Episode:  274 Max Right:  0.117 Reward:  1310.0\n",
            "EPISODE WON 275\n",
            "Episode:  275 Max Right:  0.507 Reward:  5968.0\n",
            "EPISODE WON 276\n",
            "Episode:  276 Max Right:  0.501 Reward:  7401.0\n",
            "Episode:  277 Max Right:  0.125 Reward:  538.0\n",
            "Episode:  278 Max Right:  -0.04 Reward:  358.5\n",
            "Episode:  279 Max Right:  0.317 Reward:  1573.5\n",
            "EPISODE WON 280\n",
            "Episode:  280 Max Right:  0.511 Reward:  8539.0\n",
            "Episode:  281 Max Right:  -0.284 Reward:  -4.5\n",
            "Episode:  282 Max Right:  0.063 Reward:  628.0\n",
            "Episode:  283 Max Right:  0.103 Reward:  1332.0\n",
            "EPISODE WON 284\n",
            "Episode:  284 Max Right:  0.504 Reward:  5157.5\n",
            "Episode:  285 Max Right:  0.411 Reward:  1417.5\n",
            "EPISODE WON 286\n",
            "Episode:  286 Max Right:  0.515 Reward:  8486.0\n",
            "Episode:  287 Max Right:  0.135 Reward:  1288.0\n",
            "EPISODE WON 288\n",
            "Episode:  288 Max Right:  0.51 Reward:  8329.0\n",
            "Episode:  289 Max Right:  0.133 Reward:  1356.0\n",
            "Episode:  290 Max Right:  -0.011 Reward:  1368.5\n",
            "EPISODE WON 291\n",
            "Episode:  291 Max Right:  0.508 Reward:  5753.0\n",
            "EPISODE WON 292\n",
            "Episode:  292 Max Right:  0.506 Reward:  5884.0\n",
            "Episode:  293 Max Right:  0.102 Reward:  1171.5\n",
            "Episode:  294 Max Right:  0.357 Reward:  1553.5\n",
            "EPISODE WON 295\n",
            "Episode:  295 Max Right:  0.502 Reward:  6047.0\n",
            "EPISODE WON 296\n",
            "Episode:  296 Max Right:  0.507 Reward:  6211.5\n",
            "Episode:  297 Max Right:  0.341 Reward:  1977.5\n",
            "EPISODE WON 298\n",
            "Episode:  298 Max Right:  0.523 Reward:  8289.5\n",
            "EPISODE WON 299\n",
            "Episode:  299 Max Right:  0.505 Reward:  5824.0\n",
            "Episode:  300 Max Right:  -0.012 Reward:  708.0\n",
            "EPISODE WON 301\n",
            "Episode:  301 Max Right:  0.505 Reward:  6363.0\n",
            "Episode:  302 Max Right:  0.432 Reward:  1380.5\n",
            "Episode:  303 Max Right:  0.061 Reward:  1385.0\n",
            "Episode:  304 Max Right:  0.472 Reward:  2465.0\n",
            "EPISODE WON 305\n",
            "Episode:  305 Max Right:  0.503 Reward:  8190.5\n",
            "EPISODE WON 306\n",
            "Episode:  306 Max Right:  0.505 Reward:  6830.0\n",
            "EPISODE WON 307\n",
            "Episode:  307 Max Right:  0.517 Reward:  8224.0\n",
            "Episode:  308 Max Right:  0.375 Reward:  1373.0\n",
            "EPISODE WON 309\n",
            "Episode:  309 Max Right:  0.501 Reward:  6584.5\n",
            "Episode:  310 Max Right:  0.388 Reward:  2002.5\n",
            "Episode:  311 Max Right:  0.001 Reward:  801.0\n",
            "Episode:  312 Max Right:  0.369 Reward:  2407.5\n",
            "Episode:  313 Max Right:  0.112 Reward:  915.5\n",
            "Episode:  314 Max Right:  0.217 Reward:  607.5\n",
            "Episode:  315 Max Right:  0.242 Reward:  989.5\n",
            "Episode:  316 Max Right:  0.105 Reward:  1224.5\n",
            "EPISODE WON 317\n",
            "Episode:  317 Max Right:  0.507 Reward:  6377.0\n",
            "Episode:  318 Max Right:  0.164 Reward:  1410.5\n",
            "Episode:  319 Max Right:  0.085 Reward:  904.5\n",
            "Episode:  320 Max Right:  -0.026 Reward:  429.0\n",
            "EPISODE WON 321\n",
            "Episode:  321 Max Right:  0.509 Reward:  6772.5\n",
            "Episode:  322 Max Right:  -0.377 Reward:  -12.0\n",
            "EPISODE WON 323\n",
            "Episode:  323 Max Right:  0.504 Reward:  6624.0\n",
            "Episode:  324 Max Right:  0.334 Reward:  1619.5\n",
            "Episode:  325 Max Right:  -0.273 Reward:  207.0\n",
            "Episode:  326 Max Right:  0.39 Reward:  1973.5\n",
            "EPISODE WON 327\n",
            "Episode:  327 Max Right:  0.5 Reward:  4609.5\n",
            "EPISODE WON 328\n",
            "Episode:  328 Max Right:  0.503 Reward:  5972.0\n",
            "EPISODE WON 329\n",
            "Episode:  329 Max Right:  0.512 Reward:  7864.5\n",
            "Episode:  330 Max Right:  0.395 Reward:  1899.0\n",
            "Episode:  331 Max Right:  0.261 Reward:  1253.0\n",
            "Episode:  332 Max Right:  0.379 Reward:  2360.5\n",
            "EPISODE WON 333\n",
            "Episode:  333 Max Right:  0.508 Reward:  6418.0\n",
            "EPISODE WON 334\n",
            "Episode:  334 Max Right:  0.507 Reward:  6547.5\n",
            "Episode:  335 Max Right:  0.309 Reward:  1017.5\n",
            "Episode:  336 Max Right:  0.076 Reward:  622.0\n",
            "Episode:  337 Max Right:  0.085 Reward:  1241.0\n",
            "EPISODE WON 338\n",
            "Episode:  338 Max Right:  0.507 Reward:  6691.5\n",
            "EPISODE WON 339\n",
            "Episode:  339 Max Right:  0.505 Reward:  6249.0\n",
            "EPISODE WON 340\n",
            "Episode:  340 Max Right:  0.505 Reward:  6910.0\n",
            "EPISODE WON 341\n",
            "Episode:  341 Max Right:  0.511 Reward:  7954.0\n",
            "Episode:  342 Max Right:  0.033 Reward:  1178.5\n",
            "EPISODE WON 343\n",
            "Episode:  343 Max Right:  0.51 Reward:  6013.5\n",
            "EPISODE WON 344\n",
            "Episode:  344 Max Right:  0.502 Reward:  5337.5\n",
            "Episode:  345 Max Right:  0.19 Reward:  1459.5\n",
            "Episode:  346 Max Right:  0.367 Reward:  1552.5\n",
            "Episode:  347 Max Right:  0.201 Reward:  1845.0\n",
            "EPISODE WON 348\n",
            "Episode:  348 Max Right:  0.505 Reward:  7536.5\n",
            "Episode:  349 Max Right:  -0.058 Reward:  520.0\n",
            "Episode:  350 Max Right:  -0.019 Reward:  352.5\n",
            "Episode:  351 Max Right:  -0.075 Reward:  940.0\n",
            "Episode:  352 Max Right:  0.069 Reward:  1582.0\n",
            "Episode:  353 Max Right:  0.494 Reward:  2695.0\n",
            "Episode:  354 Max Right:  0.061 Reward:  1050.0\n",
            "Episode:  355 Max Right:  -0.206 Reward:  604.0\n",
            "Episode:  356 Max Right:  0.206 Reward:  874.0\n",
            "Episode:  357 Max Right:  0.234 Reward:  1925.5\n",
            "Episode:  358 Max Right:  -0.109 Reward:  949.5\n",
            "EPISODE WON 359\n",
            "Episode:  359 Max Right:  0.505 Reward:  6167.0\n",
            "Episode:  360 Max Right:  -0.005 Reward:  1010.0\n",
            "EPISODE WON 361\n",
            "Episode:  361 Max Right:  0.509 Reward:  6237.0\n",
            "EPISODE WON 362\n",
            "Episode:  362 Max Right:  0.502 Reward:  6196.0\n",
            "EPISODE WON 363\n",
            "Episode:  363 Max Right:  0.504 Reward:  5817.5\n",
            "EPISODE WON 364\n",
            "Episode:  364 Max Right:  0.509 Reward:  6353.5\n",
            "EPISODE WON 365\n",
            "Episode:  365 Max Right:  0.512 Reward:  8395.5\n",
            "Episode:  366 Max Right:  0.283 Reward:  1429.0\n",
            "EPISODE WON 367\n",
            "Episode:  367 Max Right:  0.517 Reward:  8555.0\n",
            "Episode:  368 Max Right:  0.368 Reward:  1215.5\n",
            "Episode:  369 Max Right:  0.191 Reward:  1301.5\n",
            "EPISODE WON 370\n",
            "Episode:  370 Max Right:  0.518 Reward:  8411.5\n",
            "Episode:  371 Max Right:  -0.013 Reward:  1032.5\n",
            "EPISODE WON 372\n",
            "Episode:  372 Max Right:  0.505 Reward:  6368.0\n",
            "Episode:  373 Max Right:  0.096 Reward:  881.0\n",
            "Episode:  374 Max Right:  0.273 Reward:  759.0\n",
            "EPISODE WON 375\n",
            "Episode:  375 Max Right:  0.524 Reward:  7849.5\n",
            "EPISODE WON 376\n",
            "Episode:  376 Max Right:  0.519 Reward:  8470.0\n",
            "EPISODE WON 377\n",
            "Episode:  377 Max Right:  0.508 Reward:  6034.5\n",
            "Episode:  378 Max Right:  0.237 Reward:  1403.0\n",
            "Episode:  379 Max Right:  0.066 Reward:  935.5\n",
            "Episode:  380 Max Right:  -0.035 Reward:  720.5\n",
            "Episode:  381 Max Right:  0.358 Reward:  1900.0\n",
            "Episode:  382 Max Right:  0.272 Reward:  2000.0\n",
            "EPISODE WON 383\n",
            "Episode:  383 Max Right:  0.505 Reward:  6866.0\n",
            "Episode:  384 Max Right:  0.453 Reward:  2761.0\n",
            "Episode:  385 Max Right:  0.191 Reward:  1645.5\n",
            "Episode:  386 Max Right:  0.388 Reward:  2046.0\n",
            "Episode:  387 Max Right:  0.349 Reward:  1981.5\n",
            "EPISODE WON 388\n",
            "Episode:  388 Max Right:  0.513 Reward:  8727.0\n",
            "EPISODE WON 389\n",
            "Episode:  389 Max Right:  0.509 Reward:  6203.0\n",
            "EPISODE WON 390\n",
            "Episode:  390 Max Right:  0.508 Reward:  5703.0\n",
            "Episode:  391 Max Right:  0.224 Reward:  1726.0\n",
            "Episode:  392 Max Right:  0.396 Reward:  2464.0\n",
            "EPISODE WON 393\n",
            "Episode:  393 Max Right:  0.501 Reward:  4432.5\n",
            "EPISODE WON 394\n",
            "Episode:  394 Max Right:  0.504 Reward:  5962.5\n",
            "EPISODE WON 395\n",
            "Episode:  395 Max Right:  0.511 Reward:  8859.0\n",
            "EPISODE WON 396\n",
            "Episode:  396 Max Right:  0.506 Reward:  5911.0\n",
            "Episode:  397 Max Right:  0.47 Reward:  2309.5\n",
            "Episode:  398 Max Right:  0.111 Reward:  949.5\n",
            "Episode:  399 Max Right:  0.312 Reward:  2028.0\n",
            "EPISODE WON 400\n",
            "Episode:  400 Max Right:  0.506 Reward:  6215.0\n",
            "EPISODE WON 401\n",
            "Episode:  401 Max Right:  0.503 Reward:  5848.0\n",
            "Episode:  402 Max Right:  0.437 Reward:  2433.0\n",
            "EPISODE WON 403\n",
            "Episode:  403 Max Right:  0.504 Reward:  6531.0\n",
            "EPISODE WON 404\n",
            "Episode:  404 Max Right:  0.502 Reward:  8511.5\n",
            "EPISODE WON 405\n",
            "Episode:  405 Max Right:  0.508 Reward:  6324.0\n",
            "EPISODE WON 406\n",
            "Episode:  406 Max Right:  0.5 Reward:  3947.5\n",
            "EPISODE WON 407\n",
            "Episode:  407 Max Right:  0.5 Reward:  3817.0\n",
            "EPISODE WON 408\n",
            "Episode:  408 Max Right:  0.51 Reward:  6721.0\n",
            "EPISODE WON 409\n",
            "Episode:  409 Max Right:  0.505 Reward:  6481.0\n",
            "EPISODE WON 410\n",
            "Episode:  410 Max Right:  0.507 Reward:  5954.5\n",
            "EPISODE WON 411\n",
            "Episode:  411 Max Right:  0.5 Reward:  3724.5\n",
            "EPISODE WON 412\n",
            "Episode:  412 Max Right:  0.501 Reward:  6944.5\n",
            "EPISODE WON 413\n",
            "Episode:  413 Max Right:  0.504 Reward:  6780.5\n",
            "EPISODE WON 414\n",
            "Episode:  414 Max Right:  0.505 Reward:  6178.0\n",
            "EPISODE WON 415\n",
            "Episode:  415 Max Right:  0.506 Reward:  5778.5\n",
            "EPISODE WON 416\n",
            "Episode:  416 Max Right:  0.501 Reward:  5981.0\n",
            "EPISODE WON 417\n",
            "Episode:  417 Max Right:  0.506 Reward:  6150.5\n",
            "Episode:  418 Max Right:  0.368 Reward:  2511.5\n",
            "Episode:  419 Max Right:  0.373 Reward:  1959.0\n",
            "Episode:  420 Max Right:  0.219 Reward:  1957.5\n",
            "EPISODE WON 421\n",
            "Episode:  421 Max Right:  0.502 Reward:  6456.0\n",
            "Episode:  422 Max Right:  0.305 Reward:  1318.0\n",
            "EPISODE WON 423\n",
            "Episode:  423 Max Right:  0.515 Reward:  8571.0\n",
            "EPISODE WON 424\n",
            "Episode:  424 Max Right:  0.503 Reward:  6777.0\n",
            "EPISODE WON 425\n",
            "Episode:  425 Max Right:  0.506 Reward:  6498.0\n",
            "EPISODE WON 426\n",
            "Episode:  426 Max Right:  0.501 Reward:  5659.0\n",
            "EPISODE WON 427\n",
            "Episode:  427 Max Right:  0.502 Reward:  6861.5\n",
            "EPISODE WON 428\n",
            "Episode:  428 Max Right:  0.503 Reward:  5750.5\n",
            "EPISODE WON 429\n",
            "Episode:  429 Max Right:  0.508 Reward:  6361.5\n",
            "EPISODE WON 430\n",
            "Episode:  430 Max Right:  0.52 Reward:  7983.0\n",
            "EPISODE WON 431\n",
            "Episode:  431 Max Right:  0.5 Reward:  3274.0\n",
            "EPISODE WON 432\n",
            "Episode:  432 Max Right:  0.513 Reward:  8519.0\n",
            "Episode:  433 Max Right:  0.352 Reward:  1698.5\n",
            "EPISODE WON 434\n",
            "Episode:  434 Max Right:  0.501 Reward:  5553.0\n",
            "EPISODE WON 435\n",
            "Episode:  435 Max Right:  0.504 Reward:  7236.5\n",
            "EPISODE WON 436\n",
            "Episode:  436 Max Right:  0.502 Reward:  8295.0\n",
            "EPISODE WON 437\n",
            "Episode:  437 Max Right:  0.507 Reward:  6190.5\n",
            "EPISODE WON 438\n",
            "Episode:  438 Max Right:  0.509 Reward:  6136.0\n",
            "Episode:  439 Max Right:  0.187 Reward:  1479.5\n",
            "Episode:  440 Max Right:  0.033 Reward:  1251.5\n",
            "EPISODE WON 441\n",
            "Episode:  441 Max Right:  0.504 Reward:  6718.5\n",
            "Episode:  442 Max Right:  -0.04 Reward:  927.5\n",
            "Episode:  443 Max Right:  0.471 Reward:  2742.0\n",
            "EPISODE WON 444\n",
            "Episode:  444 Max Right:  0.51 Reward:  8717.5\n",
            "EPISODE WON 445\n",
            "Episode:  445 Max Right:  0.505 Reward:  5896.5\n",
            "EPISODE WON 446\n",
            "Episode:  446 Max Right:  0.505 Reward:  6838.5\n",
            "EPISODE WON 447\n",
            "Episode:  447 Max Right:  0.5 Reward:  3411.5\n",
            "EPISODE WON 448\n",
            "Episode:  448 Max Right:  0.518 Reward:  8377.5\n",
            "Episode:  449 Max Right:  0.268 Reward:  1492.0\n",
            "EPISODE WON 450\n",
            "Episode:  450 Max Right:  0.506 Reward:  6344.0\n",
            "EPISODE WON 451\n",
            "Episode:  451 Max Right:  0.511 Reward:  8403.5\n",
            "EPISODE WON 452\n",
            "Episode:  452 Max Right:  0.501 Reward:  5331.5\n",
            "EPISODE WON 453\n",
            "Episode:  453 Max Right:  0.508 Reward:  6061.5\n",
            "EPISODE WON 454\n",
            "Episode:  454 Max Right:  0.502 Reward:  6902.5\n",
            "EPISODE WON 455\n",
            "Episode:  455 Max Right:  0.501 Reward:  3601.5\n",
            "EPISODE WON 456\n",
            "Episode:  456 Max Right:  0.502 Reward:  6634.0\n",
            "EPISODE WON 457\n",
            "Episode:  457 Max Right:  0.501 Reward:  5889.0\n",
            "Episode:  458 Max Right:  0.162 Reward:  1402.5\n",
            "EPISODE WON 459\n",
            "Episode:  459 Max Right:  0.503 Reward:  6162.5\n",
            "EPISODE WON 460\n",
            "Episode:  460 Max Right:  0.501 Reward:  3588.5\n",
            "EPISODE WON 461\n",
            "Episode:  461 Max Right:  0.506 Reward:  6370.5\n",
            "EPISODE WON 462\n",
            "Episode:  462 Max Right:  0.506 Reward:  6306.5\n",
            "EPISODE WON 463\n",
            "Episode:  463 Max Right:  0.507 Reward:  6648.0\n",
            "EPISODE WON 464\n",
            "Episode:  464 Max Right:  0.513 Reward:  8609.0\n",
            "Episode:  465 Max Right:  0.458 Reward:  2588.0\n",
            "EPISODE WON 466\n",
            "Episode:  466 Max Right:  0.509 Reward:  5488.0\n",
            "EPISODE WON 467\n",
            "Episode:  467 Max Right:  0.514 Reward:  9040.0\n",
            "EPISODE WON 468\n",
            "Episode:  468 Max Right:  0.509 Reward:  5634.5\n",
            "EPISODE WON 469\n",
            "Episode:  469 Max Right:  0.509 Reward:  5935.5\n",
            "EPISODE WON 470\n",
            "Episode:  470 Max Right:  0.515 Reward:  8900.0\n",
            "Episode:  471 Max Right:  0.465 Reward:  2431.5\n",
            "EPISODE WON 472\n",
            "Episode:  472 Max Right:  0.512 Reward:  8144.5\n",
            "Episode:  473 Max Right:  0.41 Reward:  2548.5\n",
            "Episode:  474 Max Right:  0.444 Reward:  2494.0\n",
            "EPISODE WON 475\n",
            "Episode:  475 Max Right:  0.503 Reward:  6740.5\n",
            "EPISODE WON 476\n",
            "Episode:  476 Max Right:  0.504 Reward:  6562.5\n",
            "EPISODE WON 477\n",
            "Episode:  477 Max Right:  0.504 Reward:  6746.5\n",
            "EPISODE WON 478\n",
            "Episode:  478 Max Right:  0.509 Reward:  6169.0\n",
            "EPISODE WON 479\n",
            "Episode:  479 Max Right:  0.504 Reward:  6402.0\n",
            "EPISODE WON 480\n",
            "Episode:  480 Max Right:  0.511 Reward:  8769.0\n",
            "Episode:  481 Max Right:  0.482 Reward:  3348.5\n",
            "Episode:  482 Max Right:  0.278 Reward:  1955.5\n",
            "EPISODE WON 483\n",
            "Episode:  483 Max Right:  0.5 Reward:  5657.5\n",
            "EPISODE WON 484\n",
            "Episode:  484 Max Right:  0.503 Reward:  6791.0\n",
            "EPISODE WON 485\n",
            "Episode:  485 Max Right:  0.503 Reward:  6025.0\n",
            "EPISODE WON 486\n",
            "Episode:  486 Max Right:  0.501 Reward:  6340.0\n",
            "EPISODE WON 487\n",
            "Episode:  487 Max Right:  0.503 Reward:  6735.0\n",
            "EPISODE WON 488\n",
            "Episode:  488 Max Right:  0.504 Reward:  6336.0\n",
            "EPISODE WON 489\n",
            "Episode:  489 Max Right:  0.511 Reward:  8425.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juIQ7RlKql26"
      },
      "source": [
        "print(\"Total Time to train this model: \", round(trainingTime / 60, 3), \"Minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4nmD4vXedb4"
      },
      "source": [
        "plt.figure(1)\n",
        "plt.clf()\n",
        "plt.title('Episode v. Distance')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Distance')\n",
        "distance = [x for x in maxRightValues] \n",
        "maxRightValues_Tensor = torch.tensor(distance, dtype=torch.float)\n",
        "plt.plot(maxRightValues_Tensor.numpy())\n",
        "if len(maxRightValues_Tensor) >= 50:\n",
        "    means = maxRightValues_Tensor.unfold(0, 50, 1).mean(1).view(-1)\n",
        "    means = torch.cat((torch.zeros(50), means))\n",
        "    plt.plot(means.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdxYdUgceypW"
      },
      "source": [
        "plt.figure(2)\n",
        "plt.clf()\n",
        "plt.title('Episode v. Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "reward = [x for x in episode_rewards] \n",
        "episode_rewards_Tensor = torch.tensor(reward, dtype=torch.float)\n",
        "plt.plot(episode_rewards_Tensor.numpy())\n",
        "if len(episode_rewards_Tensor) >= 50:\n",
        "    means = episode_rewards_Tensor.unfold(0, 50, 1).mean(1).view(-1)\n",
        "    means = torch.cat((torch.zeros(50), means))\n",
        "    plt.plot(means.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEDJf8dWfQUI"
      },
      "source": [
        "plt.figure(3)\n",
        "plt.clf()\n",
        "plt.title('Episode v. TimeSteps to Reach Goal')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('TimeSteps')\n",
        "timestep_T = [x for x in episodeNum] \n",
        "episodeNum_Tensor = torch.tensor(timestep_T, dtype=torch.float)\n",
        "plt.plot(episodeNum_Tensor.numpy())\n",
        "if len(episodeNum_Tensor) >= 50:\n",
        "    means = episodeNum_Tensor.unfold(0, 50, 1).mean(1).view(-1)\n",
        "    means = torch.cat((torch.zeros(50), means))\n",
        "    plt.plot(means.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
